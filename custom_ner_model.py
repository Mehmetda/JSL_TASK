# -*- coding: utf-8 -*-
"""Custom_Ner_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BeOs7bmAqAPP-iIjbDOQYo2BSMaRrvRh
"""

import json
import os

from google.colab import files

if 'spark_jsl.json' not in os.listdir():
  license_keys = files.upload()
  os.rename(list(license_keys.keys())[0], 'spark_jsl.json')

with open('spark_jsl.json') as f:
    license_keys = json.load(f)

# Defining license key-value pairs as local variables
locals().update(license_keys)
os.environ.update(license_keys)

# Installing pyspark and spark-nlp
! pip install --upgrade -q pyspark==3.5.1  spark-nlp==$PUBLIC_VERSION

# Installing Spark NLP Healthcare
! pip install --upgrade -q spark-nlp-jsl==$JSL_VERSION  --extra-index-url https://pypi.johnsnowlabs.com/$SECRET

# Installing Spark NLP Display Library for visualization
! pip install -q spark-nlp-display

# if you want to start the session with custom params as in start function above
from pyspark.sql import SparkSession

def start(SECRET):
    builder = SparkSession.builder \
        .appName("Spark NLP Licensed") \
        .master("local[*]") \
        .config("spark.driver.memory", "16G") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .config("spark.kryoserializer.buffer.max", "2000M") \
        .config("spark.jars.packages", "com.johnsnowlabs.nlp:spark-nlp_2.12:"+PUBLIC_VERSION) \
        .config("spark.jars", "https://pypi.johnsnowlabs.com/"+SECRET+"/spark-nlp-jsl-"+JSL_VERSION+".jar")

    return builder.getOrCreate()

#spark = start(SECRET)

import json
import os

from pyspark.ml import Pipeline,PipelineModel
from pyspark.sql import SparkSession

import sparknlp_jsl
import sparknlp

from sparknlp.annotator import *
from sparknlp_jsl.annotator import *
from sparknlp.base import *


import warnings
warnings.filterwarnings('ignore')

params = {"spark.driver.memory":"16G", # Amount of memory to use for the driver process, i.e. where SparkContext is initialized
          "spark.kryoserializer.buffer.max":"2000M", # Maximum allowable size of Kryo serialization buffer, in MiB unless otherwise specified.
          "spark.driver.maxResultSize":"2000M"} # Limit of total size of serialized results of all partitions for each Spark action (e.g. collect) in bytes.
                                                # Should be at least 1M, or 0 for unlimited.

spark = sparknlp_jsl.start(license_keys['SECRET'],params=params)

print ("Spark NLP Version :", sparknlp.version())
print ("Spark NLP_JSL Version :", sparknlp_jsl.version())

spark

clinical_embeddings = WordEmbeddingsModel.pretrained('embeddings_clinical', "en", "clinical/models")\
    .setInputCols(["sentence", "token"])\
    .setOutputCol("embeddings")

def replace_entities_with_O(input_path, output_path, blacklist):
    """
    CONLL dosyasındaki blacklist'teki entity'leri 'O' ile değiştirir.
    input_path: Orijinal conll dosya yolu
    output_path: Sonuç dosyasının yolu
    blacklist: ['LOCATION', 'ROUTE', ...] gibi entity isimleri listesi
    """
    with open(input_path, "r", encoding="utf-8") as infile, \
         open(output_path, "w", encoding="utf-8") as outfile:
        for line in infile:
            # Boş satır veya başlık satırı ise aynen yaz
            if line.strip() == "" or line.startswith("-DOCSTART-"):
                outfile.write(line)
                continue

            parts = line.strip().split()
            if not parts:
                outfile.write(line)
                continue

            # Son sütun etikettir (B-LOCATION, I-ROUTE, O, vs.)
            label = parts[-1]
            # Sadece entity kısmını al (B-LOCATION -> LOCATION, I-ROUTE -> ROUTE)
            entity = label.split("-")[-1] if "-" in label else label

            if entity in blacklist and label != "O":
                parts[-1] = "O"
            outfile.write(" ".join(parts) + "\n")

replace_entities_with_O(r"/content/conll2003_text_file.conll",r"/content/filter_conll_file.rtf",["LOCATION","DURATION","ID", "FORM","DOSAGE","ROUTE", "FREQUENCY", "STRENGTH", "PROFESSION", "AGE"])

from pyspark.sql import functions as F
from sparknlp.training import CoNLL

data = CoNLL().readDataset(spark, '/content/filter_conll_file.rtf')

(data_train, data_test) = data.randomSplit([0.8, 0.2], seed = 100)
data_test = data_test.coalesce(1).withColumn("idx", F.monotonically_increasing_id())
data_train = data_train.coalesce(1).withColumn("idx", F.monotonically_increasing_id())

clinical_embeddings.transform(data_test).write.parquet('data_test.parquet')

clinical_embeddings.transform(data_train).write.parquet('data_train.parquet')

!pip install numpy==1.23.5
!pip install tensorflow==2.12.0
!pip install tensorflow-addons==0.22.0

from sparknlp_jsl.annotator import TFGraphBuilder

graph_folder_path = "medical_ner_graphs"

ner_graph_builder = TFGraphBuilder()\
    .setModelName("ner_dl")\
    .setInputCols(["sentence", "token", "embeddings"]) \
    .setLabelColumn("label")\
    .setGraphFolder(graph_folder_path)\
    .setGraphFile("auto")\
    .setHiddenUnitsNumber(24)\
    .setIsLicensed(True) # False -> if you want to use TFGraphBuilder with NerDLApproach

custom_ner_model = MedicalNerApproach()\
      .setInputCols(["sentence", "token", "embeddings"])\
      .setLabelColumn("label")\
      .setOutputCol("ner")\
      .setMaxEpochs(20)\
      .setLr(0.003)\
      .setBatchSize(10)\
      .setRandomSeed(0)\
      .setVerbose(1)\
      .setEvaluationLogExtended(True) \
      .setEnableOutputLogs(True)\
      .setIncludeConfidence(True)\
      .setTestDataset('/content/data_train.parquet')\
      .setGraphFolder(graph_folder_path)\
      .setOutputLogsPath('./ner_logs')\




ner_pipeline = Pipeline(stages=[
      clinical_embeddings,
      ner_graph_builder,
      custom_ner_model
 ])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# ner_model_custom = ner_pipeline.fit(data_train)

from sparknlp_jsl.eval import NerDLMetrics
import pyspark.sql.functions as F

pred_df = ner_model_custom.stages[2].transform(clinical_embeddings.transform(data_train))

evaler = NerDLMetrics(mode="full_chunk")

eval_result = evaler.computeMetricsFromDF(pred_df.select("label","ner"), prediction_col="ner", label_col="label", drop_o = True, case_sensitive = True).cache()

eval_result.withColumn("precision", F.round(eval_result["precision"],4))\
           .withColumn("recall", F.round(eval_result["recall"],4))\
           .withColumn("f1", F.round(eval_result["f1"],4)).show(100)

print(eval_result.selectExpr("avg(f1) as macro").show())
print (eval_result.selectExpr("sum(f1*total) as sumprod","sum(total) as sumtotal").selectExpr("sumprod/sumtotal as micro").show())