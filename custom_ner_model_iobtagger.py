# -*- coding: utf-8 -*-
"""Custom_Ner_Model-IobTagger.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o0Z7xyoTvMCpuHiymolTV4u9j0di58V_
"""

import json
import os

from google.colab import files

if 'spark_jsl.json' not in os.listdir():
  license_keys = files.upload()
  os.rename(list(license_keys.keys())[0], 'spark_jsl.json')

with open('spark_jsl.json') as f:
    license_keys = json.load(f)

# Defining license key-value pairs as local variables
locals().update(license_keys)
os.environ.update(license_keys)

# Installing pyspark and spark-nlp
! pip install --upgrade -q pyspark==3.5.1  spark-nlp==$PUBLIC_VERSION

# Installing Spark NLP Healthcare
! pip install --upgrade -q spark-nlp-jsl==$JSL_VERSION  --extra-index-url https://pypi.johnsnowlabs.com/$SECRET

# Installing Spark NLP Display Library for visualization
! pip install -q spark-nlp-display

# if you want to start the session with custom params as in start function above
from pyspark.sql import SparkSession

def start(SECRET):
    builder = SparkSession.builder \
        .appName("Spark NLP Licensed") \
        .master("local[*]") \
        .config("spark.driver.memory", "16G") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .config("spark.kryoserializer.buffer.max", "2000M") \
        .config("spark.jars.packages", "com.johnsnowlabs.nlp:spark-nlp_2.12:"+PUBLIC_VERSION) \
        .config("spark.jars", "https://pypi.johnsnowlabs.com/"+SECRET+"/spark-nlp-jsl-"+JSL_VERSION+".jar")

    return builder.getOrCreate()

#spark = start(SECRET)

import json
import os

from pyspark.ml import Pipeline,PipelineModel
from pyspark.sql import SparkSession

import sparknlp_jsl
import sparknlp

from sparknlp.annotator import *
from sparknlp_jsl.annotator import *
from sparknlp.base import *


import warnings
warnings.filterwarnings('ignore')

params = {"spark.driver.memory":"16G", # Amount of memory to use for the driver process, i.e. where SparkContext is initialized
          "spark.kryoserializer.buffer.max":"2000M", # Maximum allowable size of Kryo serialization buffer, in MiB unless otherwise specified.
          "spark.driver.maxResultSize":"2000M"} # Limit of total size of serialized results of all partitions for each Spark action (e.g. collect) in bytes.
                                                # Should be at least 1M, or 0 for unlimited.

spark = sparknlp_jsl.start(license_keys['SECRET'],params=params)

print ("Spark NLP Version :", sparknlp.version())
print ("Spark NLP_JSL Version :", sparknlp_jsl.version())

spark

clinical_embeddings = WordEmbeddingsModel.pretrained('embeddings_clinical', "en", "clinical/models")\
    .setInputCols(["sentence", "token"])\
    .setOutputCol("embeddings")

from sparknlp.training import CoNLL

training_data = CoNLL().readDataset(spark, '/content/conll2003_text_file.conll')

training_data.show(3)

# NerConverter ile named_entity'yi chunk'a çevir
ner_converter = NerConverterInternal() \
    .setInputCols(["sentence", "token", "label"]) \
    .setOutputCol("ner_chunk")\
    .setBlackList(["DURATION","ROUTE","FREQUENCY","STRENGTH","PROFESSION","LOCATION","AGE"])

pipeline = Pipeline(stages=[ner_converter])

result = pipeline.fit(training_data).transform(training_data)

result.show()

iobTagger = IOBTagger()\
  .setInputCols(["token", "ner_chunk"])\
  .setOutputCol("ner_label")

pipeline = Pipeline(stages=[iobTagger])


result_iob_tagger = pipeline.fit(result).transform(result)

result_iob_tagger.show()

result_iob_tagger.selectExpr("explode(ner_chunk) as a") \
  .selectExpr("a.begin",
              "a.end",
              "a.result as ner_chunk",
              "a.metadata.entity as ner_label").show(50, False)

row_count = result_iob_tagger.count()

result_iob_tagger.selectExpr("explode(ner_label) as a") \
  .selectExpr("a.begin",
              "a.end",
              "a.metadata.word as word",
              "a.result as chunk").show(row_count, truncate=False)

(training_data, test_data) = result_iob_tagger.randomSplit([0.8, 0.2], seed = 100)

# save the test data as parquet for easy testing
clinical_embeddings.transform(training_data).write.parquet('training_data.parquet')

clinical_embeddings.transform(test_data).write.parquet('test_data.parquet')

!pip install numpy==1.23.5
!pip install tensorflow==2.12.0
!pip install tensorflow-addons==0.22.0

from sparknlp_jsl.annotator import TFGraphBuilder

graph_folder_path = "medical_ner_graphs"

ner_graph_builder = TFGraphBuilder()\
    .setModelName("ner_dl")\
    .setInputCols(["sentence", "token", "embeddings"]) \
    .setLabelColumn("label")\
    .setGraphFolder(graph_folder_path)\
    .setGraphFile("auto")\
    .setHiddenUnitsNumber(24)\
    .setIsLicensed(True) # False -> if you want to use TFGraphBuilder with NerDLApproach

custom_ner_model = MedicalNerApproach()\
      .setInputCols(["sentence", "token", "embeddings"])\
      .setLabelColumn("ner_label")\
      .setOutputCol("ner")\
      .setMaxEpochs(20)\
      .setLr(0.003)\
      .setBatchSize(10)\
      .setRandomSeed(0)\
      .setVerbose(1)\
      .setEvaluationLogExtended(True) \
      .setEnableOutputLogs(True)\
      .setIncludeConfidence(True)\
      .setTestDataset('/content/test_data.parquet')\
      .setGraphFolder(graph_folder_path)\
      .setOutputLogsPath('./ner_logs')\




ner_pipeline = Pipeline(stages=[
      clinical_embeddings,
      ner_graph_builder,
      custom_ner_model
 ])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# ner_model_custom = ner_pipeline.fit(training_data)

from sparknlp_jsl.eval import NerDLMetrics
import pyspark.sql.functions as F

pred_df = ner_model_custom.stages[2].transform(clinical_embeddings.transform(training_data))

evaler = NerDLMetrics(mode="full_chunk")

eval_result = evaler.computeMetricsFromDF(
    pred_df.select("label", "ner"),
    prediction_col="ner",
    label_col="label",
    drop_o=True,
    case_sensitive=True
).cache()

# Sıfır f1 skoru olan entity'leri filtrele
filtered_eval_result = eval_result.filter(eval_result.f1 > 0.00001)

filtered_eval_result = filtered_eval_result.withColumn("precision", F.round(filtered_eval_result["precision"], 4)) \
                                           .withColumn("recall", F.round(filtered_eval_result["recall"], 4)) \
                                           .withColumn("f1", F.round(filtered_eval_result["f1"], 4))

filtered_eval_result.show(100)

print(filtered_eval_result.selectExpr("avg(f1) as macro").show())
print(filtered_eval_result.selectExpr("sum(f1*total) as sumprod", "sum(total) as sumtotal").selectExpr("sumprod/sumtotal as micro").show())